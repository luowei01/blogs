@[TOC](神经网络的简易搭建)
# 0.准备


```python
#让jupyterx显示一个代码块的完整结果，而不是仅显示最后一行结果
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"
import numpy as np
import torch
import torch.nn as nn #神经网络可以通过torch.nn创建
import torch.nn.functional as F #里面存放着一些常用的函数：非线性、卷积、激活、池化函数等
```

# 1.定义一个神经网络
一个 nn.Module 包括层和一个方法 forward(input) 它会返回输出(output)。


```python
class Net(nn.Module):
    def __init__(self):#注意两侧均是双下划线，否则不能成功创建
        super(Net,self).__init__()#注意两侧均是双下划线
        #卷积层
        self.conv1 = nn.Conv2d(1, 6, 5)
        self.conv2 = nn.Conv2d(6,16,5)
        #线性层、全连接层
        self.fc1 = nn.Linear(16*5*5,120)
        self.fc2 = nn.Linear(120,84)
        self.fc3 = nn.Linear(84,10)
    def forward(self, x):#迭代处理输入，前向传播
        x = F.max_pool2d(F.relu(self.conv1(x)),(2,2))
        x = F.max_pool2d(F.relu(self.conv2(x)),2)
        x = x.view(-1,16*5*5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x
net = Net() #创建一个神经网络
net
```




    Net(
      (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
      (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
      (fc1): Linear(in_features=400, out_features=120, bias=True)
      (fc2): Linear(in_features=120, out_features=84, bias=True)
      (fc3): Linear(in_features=84, out_features=10, bias=True)
    )



# 2.用神经网络处理输入


```python
#随机生成一个32*32的输入,用神经网络处理后输出
input = torch.randn(1,1,32,32) 
output = net(input)
output
net.zero_grad() #清零，防止累积
print(net.conv1.bias.grad) # 查看梯度
output.backward(torch.randn(1, 10),retain_graph=True) # 非标量，用随机的梯度来反向传播，设置retain_graph = True保留计算图，方便二次求导
print(net.conv1.bias.grad)
```




    tensor([[-0.0999,  0.0969, -0.1009,  0.0584, -0.1089, -0.0850,  0.0982, -0.0189,
              0.0397,  0.0378]], grad_fn=<AddmmBackward0>)
    None
    tensor([ 0.1001, -0.0829, -0.0468, -0.0440, -0.0682, -0.0460])
    

# 3.计算损失


```python
target = torch.randn(10).view(1,-1) # 目标值
criterion = nn.MSELoss()#用均方误差函数计算模型输出和目标值的距离即损失
loss = criterion(output,target) 
loss
```




    tensor(0.5602, grad_fn=<MseLossBackward0>)



# 4.反向传播损失
调用.grad_fn跟随损失可以得到反向传播路径：input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d -> view -> linear -> relu -> linear -> relu -> linear-> MSELoss-> loss

```python
loss.grad_fn  # MSELoss
loss.grad_fn.next_functions[0][0]  # Linear
loss.grad_fn.next_functions[0][0].next_functions[0][0]  # ReLU...
```




    <MseLossBackward0 at 0x157a01033a0>
    <AddmmBackward0 at 0x15790cb8760>
    <AccumulateGrad at 0x157a0103040>


调用loss.backward()反向传播损失,调用.bias.grad查看偏置项梯度
若是二次调用backward()求导，需要将上一次求导设置为retain_graph=True来保留计算图，防止被自动丢弃

```python
net.zero_grad()     # 先清空现存的梯度，防止累积
net.conv1.bias.grad #查看backward前conv1层的偏置项的梯度
loss.backward()
net.conv1.bias.grad #查看backward后conv1层的偏置项的梯度
```




    tensor([0., 0., 0., 0., 0., 0.])
    tensor([-0.0052,  0.0105,  0.0134,  0.0062,  0.0052, -0.0055])



# 5.更新神经网络的权重参数
一个模型可训练的参数可以通过调用 net.parameters() 返回：

```python
params = net.parameters()
for i in params:
    i.detach().numpy().shape
```




    (6, 1, 5, 5)
    (6,)
    (16, 6, 5, 5)
    (16,)
    (120, 400)
    (120,)
    (84, 120)
    (84,)
    (10, 84)
    (10,)



## 方法1：简单的随机梯度下降法
weight = weight - learning_rate * gradient


```python
learning_rate = 0.01
for i in range(10):
#     net.zero_grad() #注释掉优化的快一点，原因未知
    output = net(input)
    loss = criterion(output, target)
    loss.backward()
    for f in net.parameters():
        f = f.data.sub_(f.grad.data * learning_rate)
    print(loss)
```

    tensor(0.5602, grad_fn=<MseLossBackward0>)
    tensor(0.5236, grad_fn=<MseLossBackward0>)
    tensor(0.4839, grad_fn=<MseLossBackward0>)
    tensor(0.4433, grad_fn=<MseLossBackward0>)
    tensor(0.4004, grad_fn=<MseLossBackward0>)
    tensor(0.3547, grad_fn=<MseLossBackward0>)
    tensor(0.3026, grad_fn=<MseLossBackward0>)
    tensor(0.2366, grad_fn=<MseLossBackward0>)
    tensor(0.1586, grad_fn=<MseLossBackward0>)
    tensor(0.0790, grad_fn=<MseLossBackward0>)
    

## 方法2：使用torch.optim里的更新规则
torch.optim 实现了所有的更新规则方法，类似于 SGD, Nesterov-SGD, Adam, RMSProp等。


```python
import torch.optim as optim

# create your optimizer
optimizer = optim.SGD(net.parameters(), lr=0.01)

# in your training loop:
for i in range(10):
#     net.zero_grad() # 注释掉优化的快一点，原因未知
    optimizer.zero_grad()   # zero the gradient buffers
    output = net(input)
    loss = criterion(output, target)
    loss.backward()
    optimizer.step()    # Does the update
    print(loss)
```

    tensor(0.0320, grad_fn=<MseLossBackward0>)
    tensor(0.0290, grad_fn=<MseLossBackward0>)
    tensor(0.0263, grad_fn=<MseLossBackward0>)
    tensor(0.0238, grad_fn=<MseLossBackward0>)
    tensor(0.0216, grad_fn=<MseLossBackward0>)
    tensor(0.0196, grad_fn=<MseLossBackward0>)
    tensor(0.0177, grad_fn=<MseLossBackward0>)
    tensor(0.0161, grad_fn=<MseLossBackward0>)
    tensor(0.0146, grad_fn=<MseLossBackward0>)
    tensor(0.0133, grad_fn=<MseLossBackward0>)
