autograd软件包为所有基于tensor的操作提供自动微分，以代码运行方式定义后向传播，并且每次迭代都不同。
0.准备

```python
import torch
#让jupyterx显示一个代码块的完整结果，而不是仅显示最后一行结果
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"
```
1.将tensor的属性.requireds_grad设置为True，跟踪针对tensor的所有操作

```python
x = torch.tensor([[2.,3.],[2.,3.]], requires_grad=True)
x
```
    tensor([[2., 3.],
            [2., 3.]], requires_grad=True)

2.调用.backward()自动计算所有梯度，该张量的梯度将累积到.grad属性。
* 若是二次调用backward()求导，需要将上一次求导设置为retain_graph=True来保留计算图，防止被自自动丢弃，导致二次求导报错。
* 如果该张量(输出的结果）是标量不需要指定参数，否则backward（）需要用一个gradient参数指定张量形状。
```python
y = x+2
z = y*y*3
out = z.mean() # out = sum(3(x+2)^2)/4
out.backward() #求导后3/2*（x+2)
x
y
out
x.grad
```
    tensor([[2., 3.],
            [2., 3.]], requires_grad=True)
    tensor([[4., 5.],
            [4., 5.]], grad_fn=<AddBackward0>)
    tensor(61.5000, grad_fn=<MeanBackward0>)
    tensor([[6.0000, 7.5000],
            [6.0000, 7.5000]])


3.调用.detach()来停止对tensor历史记录的跟踪，将其与计算历史记录分离并防止将来的计算被跟踪
Tensor 和 Function 互相连接并构建一个非循环图，它保存整个完整的计算过程的历史信息。每个张量都有一个 .grad_fn 属性保存着创建了张量的 Function 的引用

```python
out
out.detach()#无grad_fn
```
    tensor(61.5000, grad_fn=<MeanBackward0>)
    tensor(61.5000)

4.评估模型时不需要梯度，将代码块用with torch.no_grad()包装，将停止跟踪历史记录和使用内存

```python
(x**2).requires_grad
with torch.no_grad():
    (x**2).requires_grad
```

    True
    False


