```python
from torch.utils.data import DataLoader
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision.transforms import Compose,ToTensor,Normalize
from torchvision.datasets import MNIST
import os,time
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
```


```python
"""准备数据集"""
def get_dataloader(train = True,batch_size = 128):
    transform_function = Compose([ToTensor(),Normalize(mean=(0.1307,),std=(0.3081,))])
    dataset = MNIST(root="C:/Users/luoweu/Desktop/pytorch学习/DataSet/MINST",\
                   download=True,train=train,transform=transform_function)
    dataloader = DataLoader(dataset=dataset,batch_size=batch_size,shuffle=True,num_workers=4,pin_memory=True)
    return dataloader
train_data= get_dataloader(train=True,batch_size=128)
test_data = get_dataloader(train=False,batch_size=6000)
"""构建模型"""
class mymodel(nn.Module):#三层网络，两个全连接层，输入、输出层
    def __init__(self):
        super(mymodel,self).__init__()
        #输入：batch_size*1*28*28
        self.layer1=nn.Linear(28*28,28)#全连接层
        #输出batch_size*28
        self.layer2=nn.Linear(28,10)#全连接层
        #输出batch_size*10
    def forward(self,x):
        x = x.view(-1,28*28)
        x = self.layer1(x)
        x = nn.functional.relu(x)#激活函数
        x = self.layer2(x)
        return x
model = mymodel()
model.to(device)
"""多分类使用交叉熵损失
        计算过程
        将得到的结果softmax(x(i)_new = [exp(x(i))]/[exp(x(1)+....+exp(x(10)])变为概率，再取对数求和，
        （取负号）乘以真实值，得到单个样本损失，再求一个batch_size的平均损失
        """
# 方法一      
criterion=nn.CrossEntropyLoss()
# loss = criterion(Y_predict,Y_true)
# 方法二
# out = nn.functional.log_softmax(Y_predict,dim=-128)#softmax,再取对数
# loss = nn.functional.nll_loss(Y_predict,Y_true)#将真实值作为权重，求带权损失
"""选择优化器"""
optimizer = optim.Adam(model.parameters(),lr=0.001)
```


```python
"""模型的加载"""
if os.path.exists("runs/model/minst_net.pt"):
    model.load_state_dict(torch.load("runs/model/minst_net.pt"))
    optimizer.load_state_dict(torch.load("runs/model/minst_optimizer.pt"))
"""开始训练模型"""
start = time.time()
model.train(mode=True)
for epoch in range(5):
    for i,(data,target) in enumerate(train_data):
        optimizer.zero_grad()
        data = data.to(device) 
        predict = model(data)
        target = target.to(device) 
        loss = criterion(predict,target)
        loss.backward()
        optimizer.step()
        if i%10 == 0:
            print(f"Train epoch: {epoch} [{i*len(data)}/{len(train_data.dataset)} ({100.*i/len(train_data):.1f}%)]\tLoss: {loss.item()}")
print(f"训练耗时{time.time()-start}s")
"""模型的保存"""
os.makedirs("runs/model", exist_ok=True)
torch.save(model.state_dict(),"runs/model/minst_net.pt")#保存模型参数
torch.save(optimizer.state_dict(),"runs/model/minst_optimizer.pt")#保存优化器参数
```


```python
"""模型的加载"""
model.load_state_dict(torch.load("runs/model/minst_net.pt"))
optimizer.load_state_dict(torch.load("runs/model/minst_optimizer.pt"))
"""模型的评估"""
model.eval()
loss = 0
correct=0
for (data,target) in test_data:
    with torch.no_grad():
        data = data.to(device)  
        out = model(data)
        target = target.to(device)
        loss += nn.functional.nll_loss(out,target,reduction="sum").item()
        pred= out.data.max(-1,keepdim=True)[-1]#求最后一个维度的最大值索引
        correct += pred.eq(target.data.view_as(pred)).sum().item()
print(f"平均损失：{loss/len(test_data.dataset):.6f}\t准确率:{100*correct/len(test_data.dataset):.2f}%")
```

    平均损失：-10.738708	准确率:96.06%
    


```python

```
