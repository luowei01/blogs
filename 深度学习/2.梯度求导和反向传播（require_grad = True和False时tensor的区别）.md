```python
import torch 
from IPython.core.interactiveshell import InteractiveShell#输出完整信息
InteractiveShell.ast_node_interactivity = "all"
x = torch.ones((4,3),requires_grad = True)
```


```python
#以下列函数为例展示如何用grad_fn记录前向计算过程
x
y = x+2
y
z = y*y*3 
z
with torch.no_grad():#此语句包装的代码不跟踪记录历史操作，避免使用内存
    out=z.mean()
    out.requires_grad
    out
out = z.mean()#求均值
out 
```




    tensor([[1., 1., 1.],
            [1., 1., 1.],
            [1., 1., 1.],
            [1., 1., 1.]], requires_grad=True)






    tensor([[3., 3., 3.],
            [3., 3., 3.],
            [3., 3., 3.],
            [3., 3., 3.]], grad_fn=<AddBackward0>)






    tensor([[27., 27., 27.],
            [27., 27., 27.],
            [27., 27., 27.],
            [27., 27., 27.]], grad_fn=<MulBackward0>)






    False






    tensor(27.)






    tensor(27., grad_fn=<MeanBackward0>)




```python
#用backward()方法反向传播计算梯度，用grad属性查看对应的梯度
out.backward()#out是一个标量，直接调用backward（）方法即可,很多时候损失都是一个标量。每计算一次，backward只能调用一次
x.grad
x.grad.data.zero_()#除非每次计算都对x进行重新创建，否则每次计算的梯度值将会累积，需要在每一次计算后清空
x.data +=1#直接修改x的值，非重新创建，若梯度没有置0，反复计算时梯度依然会累积
```




    tensor([[1.5000, 1.5000, 1.5000],
            [1.5000, 1.5000, 1.5000],
            [1.5000, 1.5000, 1.5000],
            [1.5000, 1.5000, 1.5000]])






    tensor([[0., 0., 0.],
            [0., 0., 0.],
            [0., 0., 0.],
            [0., 0., 0.]])



# 总结
    （1）在tensor中使用requie_grad=True来记录与tensor相关的每一步计算操作，保存在对应变量的grad_fn属性中。
    （2）每进行一次整个前向计算过程时，backword()只能调用一次，求出的梯度结果会保存在最初的tensor.grad属性中。
    （3）grad中保存的梯度是会累积的，故每进行一次计算需要清零：方法1是x.grad.data.zero_()，方法2是重新创建x,若不重新创建x,仅用x.data对x   的值进行修改，不影响梯度计算，但仍然会累积。
    （4）注意当tensor添加了requie_grad=True时，用tensor.data才可以获取到原始的tensor（requie_grad=false时），使用相关的类型转化方法。否则相当于依然在进行计算操作的记录，需要使用如tensor.detach().numpy()相关的方法才可以。
